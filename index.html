<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Paper read summary for MapReduce by hgm900611</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/main.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>

      <header>
        <h1>Paper read summary for MapReduce</h1>
        <p></p>
      </header>

      <div id="banner">
        <span id="logo"></span>

        <a href="https://github.com/hgm900611/paper_1" class="button fork"><strong>View On GitHub</strong></a>
        <div class="downloads">
          <span>Downloads:</span>
          <ul>
            <li><a href="https://github.com/hgm900611/paper_1/zipball/master" class="button">ZIP</a></li>
            <li><a href="https://github.com/hgm900611/paper_1/tarball/master" class="button">TAR</a></li>
          </ul>
        </div>
      </div><!-- end banner -->

    <div class="wrapper">
      <nav>
        <ul></ul>
      </nav>
      <section>
        <blockquote>
<p>This page includes my paper read summary for MapReduce. I will mainly talks about three important issues including:</p>

<ul>
<li><strong>Job scheduling for minimizing the total response time</strong></li>
<li><strong>Data locality issue</strong></li>
<li><strong>Speculative Execution</strong></li>
</ul>
<p>For each issue I will make two categories which are theoretical analysis based optimization and heuristic based algorithm  design. Hope you can get something useful from this summary.</p>
</blockquote>

<hr><h2>
<a name="job-scheduling" class="anchor" href="#job-scheduling"><span class="octicon octicon-link"></span></a>Job Scheduling</h2>

<h3>
<a name="theoretical-analysis-based" class="anchor" href="#theoretical-analysis-based"><span class="octicon octicon-link"></span></a>Theoretical Analysis based:</h3>

<ul>
<li>
<p><a href="http://dl.acm.org/citation.cfm?id=1989493.1989540">B. Moseley, A. Dasgupta, R. Kumar, and T. Sarlos, “On scheduling in
map-reduce and flow-shops,” in SPAA 2011.</a> This paper seeks to minimize the <em>flowtime</em> of jobs in MapReduce which is motivated from the two-stage flow shop problem. The tasks to machine mapping is unknown and needs to be determined such that the total <em>flowtime</em> is minimized. </p>

<pre><code>In our following research, we can consider to optimize the job scheduling in a heterogeneous environment where the machines are not identical.
</code></pre>
</li>
<li><p><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5935152&amp;tag=1">Hyunseok Chang, Murali Kodialam, Ramana Rao Kompella, T. V. Lakshman, Myungjin Lee, Sarit Mukherjee, "Scheduling in MapReduce-like Systems for Fast completion time," in Infocom 2011.</a> This paper seeks to optimize the total completion time of job running in the Mapreduce system. It decides the scheduling order of tasks on each processor. Moreover, this paper doesn't consider the dependency between map task and reduce tasks. A linear program is formulated to relax the original problem. It then outlines a combinatorial approach to solve the LP problem.</p></li>
<li><p><a href="http://www.cse.psu.edu/%7Efachen/pubs/infocom12mr.pdf">F. Chen, M. Kodialam, and T. Lakshman, “Joint scheduling of processing
and shuffle phases in MapReduce systems,” in Infocom 2012.</a> 
Actually this paper is the follow-up work of the above paper. To be more practical, it considers the dependency between map and reduce tasks. Moreover, it jointly optimize the job completion time by imposing the shuffle phase constraint. </p></li>
<li><p><a href="http://dl.acm.org/citation.cfm?id=2254761">Jian Tan, Xiaoqiao Meng, Li Zhang, “Delay Tails in MapReduce Scheduling,” in SIGMETRICS 2012.</a> This paper focuses on the the starvation problem of small jobs under fair scheduler in MapReduce. The root reason of this problem is caused by launching the reduce tasks so early. To efficiently solve this issue and improve the job response performance, it presents a couple scheduler which assign the reduce tasks based on the progress of the map phase. The theoretical analysis here is very complicated. </p></li>
<li><p><a href="http://newslab.ece.ohio-state.edu/research/resources/Analysis.pdf">Yousi Zheng, Ness Shroff, Prasun Sinha, “A New Analytical Technique for Designing Provably Efficient MapReduce Schedulers,” in Infocom 2013</a>. This paper is the first work to write down the formulation of mapreduce job scheduling including both map and reduce phases. There is no distinguish between map and reduce slots here. It uses adopts the knowledge in probability to prove that any work-conserving scheduler can achieves a constant efficiency ratio (not competitive ratio). Moreover, this paper presents the ASRPT algorithm which is inspired from the famous SRPT (Shortest remaining time first) algorithm and bound the competitive ratio to be 2.</p></li>
<li><p><a href="http://users.cms.caltech.edu/%7Eadamw/papers/preprint_mapreduce.pdf">M. Lin, L. Zhang, A. Wierman and J. Tan, "Joint optimization of overlapping phases in MapReduce," in IFIP 2013.</a>. This is the first work to consider the overlapping of map phase and shuffle phase so far. A nice formulation is also written down here. Hover, even the offline case with batch arrival is shown to be NP-Complete. It then presents the MaxSRPT ans SpiltSRPT algorithm which are complementary to solve schedule the jobs.</p></li>
</ul><h3>
<a name="heuristic-based" class="anchor" href="#heuristic-based"><span class="octicon octicon-link"></span></a>Heuristic based:</h3>

<blockquote>
<p>I haven't read any papers which present heuristic-based algorithm to optimize the job completion time in MapReduce system. </p>
</blockquote>

<hr><h2>
<a name="data-locality" class="anchor" href="#data-locality"><span class="octicon octicon-link"></span></a>Data locality</h2>

<h3>
<a name="theoretical-analysis-based-1" class="anchor" href="#theoretical-analysis-based-1"><span class="octicon octicon-link"></span></a>Theoretical Analysis based:</h3>

<ul>
<li><p><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6284711&amp;tag=1">Qiaomin Xie, Yi Lu, “Degree-guided map-reduce task assignment with
data locality constraint,” in ISIT 2012.</a> This paper presents the peeling algorithm and adopts the mean-field equations to analyze the evolution (I cannot understand it well). However, it makes an impractical assumption which is that all the tasks with data locality on the machine can be finished in a time slot with probability <em>p</em> while the tasks without data locality can only be finished with probability <em>q</em> and <strong>p &gt; q</strong>. Also the theoretical here is very complex. </p></li>
<li><p><a href="http://inlab.lab.asu.edu/Publications/WanZhuYin_13.pdf">W. Wang, K. Zhu, L. Ying, J. Tan, L. Zhang, "Map Task Scheduling in MapReduce with Data Locality: Throughput and Heavy-Traffic Optimality," in Infocom 2013.</a>. This paper makes the same assumption as the above paper presented in ISIT 2012. Its task scheduling algorithm includes two aspects which are<code>Routing</code> and <code>Scheduling</code>. The purpose of routing is to keep load balancing for all the machines. Also scheduling process makes a very simple decision based on the length of queues. It adopts the Lyapunov function to prove the stability of the algorithm. The analysis here is also nontrivial. </p></li>
<li><p><a href="https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CDIQFjAB&amp;url=http%3A%2F%2Fresearcher.watson.ibm.com%2Fresearcher%2Ffiles%2Fus-tanji%2FreducePlace2013.pdf&amp;ei=Uh4rUq5mrMGJB-j5gNAJ&amp;usg=AFQjCNEGqVhyg8P3Vj5uOxx7QUrrei4ZQg">J. Tan, X. Meng, L. Zhang, "Improving ReduceTask Data Locality for Sequential MapReduce Jobs," in Infocom 2013.</a> This is the first paper to analyze the data locality issue for reduce tasks with theoretical analysis. Actually, the model here is quite simple, the objective is only to minimize the data transfer cost in shuffle phase with proper reduce task placement. The data transfer cost gives a very good angle for analyzing data locality issue.  </p></li>
</ul><h3>
<a name="heuristic-based-1" class="anchor" href="#heuristic-based-1"><span class="octicon octicon-link"></span></a>Heuristic based:</h3>

<ul>
<li><p><a href="http://www.sigops.org/sosp/sosp09/papers/isard-sosp09.pdf">Michael Isard, Vijayan Prabhakaran, Jon Currey, Udi Wieder, Kunal Talwar and Andrew Goldberg, "Quincy: Fair Scheduling for Distributed Computing Clusters," in SOSP 2009.</a> This is the first paper to address the data locality issue and fairness problem in MapReduce-like systems. It encodes the scheduling as a flow network. In this network, the edge weights encode the demands of data locality and fairness. This is a very novel and beautiful work. </p></li>
<li><p><a href="http://dl.acm.org/citation.cfm?id=1755940">Matei Zaharia, Dhruba Borthakur, Joydeep Sen Sarma, Khaled Elmeleegy, Sunnyvale, Scott Shenker, Ion Stoica, "Delay scheduling: a simple technique for achieving locality and fairness in cluster scheduling," in EuroSys 2010.</a> This paper is very famous among all the papers which address the data locality issue. It presents the delay scheduling algorithm which delay the job at the head of the queue a little bit if data locality is not achieved with the available slot. As described next, this algorithm has a obvious drawback: there exists a probability that some slots wont'e be launching any tasks and keeps idle all the way. This phenomenon waste the resource in the cluster and needs to be improved. </p></li>
<li><p><a href="http://researcher.watson.ibm.com/researcher/files/us-tanji/coupling2013.pdf">J. Tan, X. Meng, L. Zhang, "Coupling Task Progress for MapReduce Resource-Aware Scheduling", in Infocom 2013.</a>.</p></li>
</ul><hr><h2>
<a name="speculative-execution" class="anchor" href="#speculative-execution"><span class="octicon octicon-link"></span></a>Speculative execution</h2>

<h3>
<a name="theoretical-analysis-based-2" class="anchor" href="#theoretical-analysis-based-2"><span class="octicon octicon-link"></span></a>Theoretical Analysis based:</h3>

<ul>
<li>
<a href="http://personal.ie.cuhk.edu.hk/%7Exh112/research/forum.pdf">Huanle Xu, Wing Cheong Lau, "Resource optimization for speculative execution in a MapReduce Cluster, in ICNP PhD Forum 2013.</a>. This paper is the first work to address the backup strategy with theoretical analysis in MapReduce System. </li>
</ul><h3>
<a name="heuristic-based-2" class="anchor" href="#heuristic-based-2"><span class="octicon octicon-link"></span></a>Heuristic based:</h3>

<ul>
<li><p><a href="http://research.google.com/archive/mapreduce-osdi04.pdf">J. Dean and S. Ghemawat, “MapReduce: Simplified Data Processing on
Large Clusters”, in USENIX OSDI, 2004.</a>
Actually, this paper is the technical report to the MapReduce system design of Google. It talks about the back up strategy with a short paragraph. The strategy is that  when a MapReduce operation is close to complete, it will randomly choose some remaining tasks and launch the duplicates of them on alternative machines. It concludes that this speculative execution scheme can decrease the job execution time by 44%.</p></li>
<li><p><a href="http://bnrg.eecs.berkeley.edu/%7Erandy/Courses/CS268.F08/papers/42_osdi_08.pdf">M. Zaharia, A. Konwinski, A. D. Joseph, R. Katz, and I. Stoica,
“Improving mapreduce performance in heterogeneous environments,” in OSDI 2008.</a> In this paper, a new strategy named LATE is presented. LATE monitors the progress rate of the tasks and estimates their remaining time. Tasks with their progress rate below <em>slowTaskTherehold</em> are chosen as backup
candidates and the one with the longest remaining time is given the highest priority when the number of backups in the cluster does not exceed <em>speculativeCap</em>.</p></li>
<li><p><a href="http://www.usenix.org/legacy/event/osdi10/tech/full_papers/Ananthanarayanan.pdf">G. Ananthanarayanan, S. Kandula, A. Greenberg, I. Stoica, Y. Lu, B.
Saha, E. Harris, “Reining in the Outliers in MapReduce Clusters using
Mantri,” in USENIX OSDI, Oct 2010.</a>
This paper talks about the backup stategy of Mantri which is a large data processing syetem in Microsoft  It estimates for each task the remaining time to finish, trem, and predicts the duration of a new copy of
the task, <code>t_new</code>. Once a slot is idle, Mantri then makes a decision whether to do backup or not based on the statistics of trem and tnew. More precisely, a duplicate is scheduled if <code>P(t_rem &gt; 2  t_new) &gt; δ</code> is satisfied. By default, δ = .25. Hence, Mantri schedules a duplicate only if the total resource consumption decreases. Moreover Mantri allows killing a task if it really takes a long time to complete the remaining work.</p></li>
<li><p><a href="http://www.computer.org/csdl/trans/tc/preprint/06419699-abs.html">Qi Chen, Chen Liu, Zhen Xiao, "Improving MapReduce Performance Using Smart Speculative Execution Strategy," in transactions of Computer Networks 2013.</a></p></li>
</ul><h3>
<a name="support-or-contact" class="anchor" href="#support-or-contact"><span class="octicon octicon-link"></span></a>Support or Contact</h3>

<blockquote>
<p>Having any problems with this summary, please feel free to contact <a href="http://personal.ie.cuhk.edu.hk/%7Exh112">me</a> with the email xh112[@]ie.cuhk.edu.hk. You all are welcome to show your different opinions. </p>
</blockquote>
      </section>
      <footer>
        <p>Project maintained by <a href="https://github.com/hgm900611">hgm900611</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
    
  </body>
</html>